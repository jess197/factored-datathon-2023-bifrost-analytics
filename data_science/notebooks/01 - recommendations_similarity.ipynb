{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52bdacb",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [1. About the Notebook](#toc1_)    \n",
    "- [2. Libraries and Packages](#toc2_)    \n",
    "- [3. Connecting In Snowflake](#toc3_)    \n",
    "- [4. Defining Functions](#toc4_)    \n",
    "  - [4.1. Text Cleaning](#toc4_1_)    \n",
    "  - [4.2. Tokenizing](#toc4_2_)    \n",
    "  - [4.3. Remove Stopwords](#toc4_3_)    \n",
    "  - [4.4. Count Words in each product and discard irrelevant products](#toc4_4_)    \n",
    "  - [4.5. Count Vectorizer](#toc4_5_)    \n",
    "  - [4.6. One Hot Encoding](#toc4_6_)    \n",
    "  - [4.7. Recommendations with WALS](#toc4_7_)    \n",
    "- [5. Loading Data and Applying all Functions](#toc5_)    \n",
    "  - [5.1. Extracting list of all Main Categories](#toc5_1_)    \n",
    "  - [5.2. Producing a Recommendation dataframe to each Main Category](#toc5_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527991b",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[1. About the Notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22eb9a",
   "metadata": {},
   "source": [
    "Notebook para exploração inicial dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f43108",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[2. Libraries and Packages](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb274799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import sigmoid_kernel\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import spacy\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "from faiss import IndexFlatIP\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.examples import sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b49b",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[3. Connecting In Snowflake](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5171891",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(\n",
    "    user='bksramos',\n",
    "    password='BifrostBKR2023',\n",
    "    account='abb76531.us-east-1',\n",
    "    warehouse='ANALYTICS_WH',\n",
    "    database='AMAZON',\n",
    "    schema='AMZ_DATA_SILVER',\n",
    "    role = 'ANALYSTS'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6d0d8",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[4. Defining Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12c2c2",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[4.1. Text Cleaning](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f257a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    # Remove HTML tags using BeautifulSoup\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text(separator=\" \")\n",
    "    \n",
    "    # Remove patterns like \\n and its variations\n",
    "    cleaned_text = re.sub(r'\\\\n+', ' ', cleaned_text)\n",
    "    \n",
    "    # Remove punctuations (excluding single quote to preserve words like \"isn't\" and \"it's\")\n",
    "    cleaned_text = re.sub(r'[^\\w\\s\\']', ' ', cleaned_text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "    # Removing t, tt and its similarities\n",
    "    cleaned_text = re.sub(r'\\bt+\\b', ' ', cleaned_text)\n",
    "\n",
    "    cleaned_text = re.sub(r'\\btt+\\b', ' ', cleaned_text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    # Strip leading and trailing whitespaces\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    #all text to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Register the UDF\n",
    "# clean_html_udf = udf(clean_html, StringType())\n",
    "\n",
    "# Register the UDF\n",
    "# clean_text_udf = udf(clean_html_udf, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd42a6",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[4.2. Tokenizing](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e566e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(column):\n",
    "    \"\"\"Tokenizes a Pandas dataframe column and returns a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        column: Pandas dataframe column (i.e. df['text']).\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): Tokenized list\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454c98e",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[4.3. Remove Stopwords](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ec03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = list(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8252cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StopWordsRemover(text):\n",
    "    '''\n",
    "    Removes Stop Words (also capitalized) from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without Stop Words\n",
    "    ''' \n",
    "    # check in lowercase \n",
    "    t = [token for token in text if token.lower() not in stopwords]\n",
    "    text = ' '.join(t)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc67b9a",
   "metadata": {},
   "source": [
    "## <a id='toc4_4_'></a>[4.4. Count Words in each product and discard irrelevant products](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_func(text):\n",
    "    '''\n",
    "    Counts words within a string\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Number of words within a string, integer\n",
    "    ''' \n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a532c3b",
   "metadata": {},
   "source": [
    "## <a id='toc4_5_'></a>[4.5. Count Vectorizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_MIN = 1\n",
    "NGRAM_MAX = 1\n",
    "MIN_DOC_FREQ = 1\n",
    "MAX_DOC_FREQ = 1.0\n",
    "MAX_TERMS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc97a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(NGRAM_MIN,NGRAM_MAX), stop_words=None, \n",
    "                             lowercase=False, max_df=MAX_DOC_FREQ, min_df=MIN_DOC_FREQ, \n",
    "                             max_features=MAX_TERMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae995f",
   "metadata": {},
   "source": [
    "## <a id='toc4_6_'></a>[4.6. One Hot Encoding](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c562681",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73da8b",
   "metadata": {},
   "source": [
    "## <a id='toc4_7_'></a>[4.7. Recommendations with WALS](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a67796",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 16\n",
    "MIN_WORDS = 32\n",
    "N_COMPONENTS = 256\n",
    "N_CLUSTERS = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9749ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wals = AlternatingLeastSquares(factors=N_COMPONENTS, regularization=0.1,\n",
    "                               iterations=15, calculate_training_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310af3f1",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[5. Loading Data and Applying all Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6c591",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[5.1. Extracting list of all Main Categories](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    MAIN_CATEGORY\n",
    "FROM \n",
    "    METADATA\n",
    "WHERE \n",
    "    FEATURE <> '[]' AND \n",
    "    PRICE IS NOT NULL AND\n",
    "    MAIN_CATEGORY IS NOT NULL AND \n",
    "    MAIN_CATEGORY <> ''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = pd.read_sql_query(query1, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff223859",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = category.MAIN_CATEGORY.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b4bf9",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[5.2. Producing a Recommendation dataframe to each Main Category](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce922d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in category_list:\n",
    "    query = \"\"\"\n",
    "    WITH CTE AS (\n",
    "        SELECT \n",
    "            MT.*,\n",
    "            REV.OVERALL,\n",
    "            AVG(REV.OVERALL) OVER (PARTITION BY MT.ASIN) AS \"PRODUCT_AVG_RATING\" \n",
    "        FROM \n",
    "            METADATA AS MT \n",
    "        INNER JOIN \n",
    "            REVIEWS AS REV ON MT.ASIN = REV.ASIN \n",
    "        WHERE \n",
    "            MAIN_CATEGORY = '{i}' AND \n",
    "            FEATURE <> '[]' AND \n",
    "            PRICE IS NOT NULL\n",
    "    )\n",
    "    SELECT\n",
    "        *\n",
    "    FROM \n",
    "        (\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY ASIN ORDER BY PRICE DESC) AS ROW_NUMBER\n",
    "    FROM\n",
    "        CTE\n",
    "    )\n",
    "    WHERE \n",
    "        ROW_NUMBER = 1 AND \n",
    "        PRODUCT_AVG_RATING >= 4\n",
    "    \"\"\".format(i=i)\n",
    "    print(i)\n",
    "    metadata = pd.read_sql_query(query, conn)\n",
    "    cols = ['TITLE', 'BRAND', 'CATEGORY', 'DETAILS', 'FEATURE']\n",
    "    metadata['full_text'] = metadata[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    metadata['full_text'] = metadata['full_text'].apply(clean_html)\n",
    "    metadata['text_tokens'] = metadata['full_text'].apply(tokenize)\n",
    "    metadata['text_tokens_sw'] = metadata['text_tokens'].apply(StopWordsRemover)\n",
    "    metadata['word_count'] = metadata['text_tokens_sw'].apply(word_count_func)\n",
    "    metadata = metadata[metadata['word_count'] > 32]  \n",
    "    if len(metadata) > 10:\n",
    "        metadata.reset_index(level=None, drop=True, inplace=True, col_level=0, col_fill='') ## reseting index\n",
    "        x_count = vectorizer.fit_transform(metadata['text_tokens_sw'])\n",
    "        x_wt = bm25_weight(x_count)\n",
    "        x_tags = enc.fit_transform(metadata[['CATEGORY','BRAND']])\n",
    "        x_tags.data = 5.0 * x_tags.data\n",
    "        x = hstack([x_wt, x_tags])\n",
    "\n",
    "        logger.info('Factorizing with WALS...')\n",
    "        wals.fit(x, show_progress=True)\n",
    "        doc_factors_wals = normalize(wals.item_factors, norm=\"l2\", axis=1, copy=False)\n",
    "        word_factors_wals = normalize(wals.user_factors, norm=\"l2\", axis=1, copy=False)\n",
    "\n",
    "        logger.info('Finding event nearest neighbors with WALS factors...')\n",
    "        K = 16 # number of recommendations for each product\n",
    "        published_idx = metadata.index.values\n",
    "        original_idx = published_idx\n",
    "        published_idx = np.array(list(range(0, len(original_idx))))\n",
    "        pub_evts = IndexFlatIP(N_COMPONENTS)\n",
    "        pub_evts.add(word_factors_wals[published_idx])\n",
    "        nn_dist, nn_idx = pub_evts.search(word_factors_wals, K+1)\n",
    "        product_neighbor = pd.DataFrame.from_dict({\n",
    "        'product_id': np.repeat(metadata['ASIN'].values, K+1),\n",
    "        'num_words': np.repeat(metadata['word_count'].values, K+1),\n",
    "        'num_words_neighbor': metadata['word_count'].values[published_idx[nn_idx.flatten()]],\n",
    "        'neighbor_id': metadata['ASIN'].values[published_idx[nn_idx.flatten()]], \n",
    "        'similarity': nn_dist.flatten()\n",
    "        })\n",
    "        same_id = product_neighbor.loc[product_neighbor['product_id'] == product_neighbor['neighbor_id']].index.values.tolist()\n",
    "        product_neighbor.drop(labels=same_id, axis='index', inplace=True)\n",
    "        product_neighbor.sort_values(['product_id','similarity'], ascending=[True, False], inplace=True)\n",
    "\n",
    "        recommendations = product_neighbor.groupby('product_id')['neighbor_id'].apply(list).reset_index(name='recommendations')\n",
    "\n",
    "        recommendations['MAIN_CATEGORY'] = '{i}'\n",
    "\n",
    "        recommendations.to_parquet(\n",
    "            path=\"../output/similarity/recommendations_{i}.parquet\".format(i=i),\n",
    "            engine=\"auto\"\n",
    "        )   \n",
    "    else:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
