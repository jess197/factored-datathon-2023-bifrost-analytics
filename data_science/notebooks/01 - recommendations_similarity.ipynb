{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52bdacb",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [1. About the Notebook](#toc1_)    \n",
    "- [2. Libraries and Packages](#toc2_)    \n",
    "- [3. Connecting In Snowflake](#toc3_)    \n",
    "- [4. Defining Functions](#toc4_)    \n",
    "  - [4.1. Text Cleaning](#toc4_1_)    \n",
    "  - [4.2. Tokenizing](#toc4_2_)    \n",
    "  - [4.3. Remove Stopwords](#toc4_3_)    \n",
    "  - [4.4. Count Words in each product and discard irrelevant products](#toc4_4_)    \n",
    "  - [4.5. Count Vectorizer](#toc4_5_)    \n",
    "  - [4.6. One Hot Encoding](#toc4_6_)    \n",
    "  - [4.7. Recommendations with WALS](#toc4_7_)    \n",
    "- [5. Loading Data and Applying all Functions](#toc5_)    \n",
    "  - [5.1. Extracting list of all Main Categories](#toc5_1_)    \n",
    "  - [5.2. Producing a Recommendation dataframe to each Main Category](#toc5_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527991b",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[1. About the Notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22eb9a",
   "metadata": {},
   "source": [
    "Notebook that creates similarity recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83dae2",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Pip install](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "75bed4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fastparquet\n",
      "  Obtaining dependency information for fastparquet from https://files.pythonhosted.org/packages/d9/0e/0018aa452cab57d804b85983e4a6b79bf95d2dcff7ff46db1976cbbdd63b/fastparquet-2023.7.0-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading fastparquet-2023.7.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /Users/jessica/Library/Python/3.9/lib/python/site-packages (from fastparquet) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/jessica/Library/Python/3.9/lib/python/site-packages (from fastparquet) (1.25.2)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Obtaining dependency information for cramjam>=2.3 from https://files.pythonhosted.org/packages/98/11/eee23c402d09642b8292f04c6cb19f39b4cfd8f622f8f1bb704e0dc1b25c/cramjam-2.7.0-cp39-cp39-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl.metadata\n",
      "  Downloading cramjam-2.7.0-cp39-cp39-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl.metadata (4.0 kB)\n",
      "Collecting fsspec (from fastparquet)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: packaging in /Users/jessica/Library/Python/3.9/lib/python/site-packages (from fastparquet) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jessica/Library/Python/3.9/lib/python/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jessica/Library/Python/3.9/lib/python/site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/jessica/Library/Python/3.9/lib/python/site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.15.0)\n",
      "Downloading fastparquet-2023.7.0-cp39-cp39-macosx_11_0_arm64.whl (584 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.2/584.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading cramjam-2.7.0-cp39-cp39-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, cramjam, fastparquet\n",
      "Successfully installed cramjam-2.7.0 fastparquet-2023.7.0 fsspec-2023.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a4422ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1586229562.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[88], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install pyarrow\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# pip install beautifulsoup4\n",
    "# pip install scikit-learn \n",
    "#pip install nltk\n",
    "#pip install spacy\n",
    "#pip install loguru\n",
    "#pip install faiss-cpu\n",
    "#pip install implicit\n",
    "#pip install --upgrade sqlalchemy\n",
    "#pip install --upgrade snowflake-sqlalchemy\n",
    "#pip install pyarrow\n",
    "#pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f43108",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[2. Libraries and Packages](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb274799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jessica/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import sigmoid_kernel\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import spacy\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "from faiss import IndexFlatIP\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.examples import sentences \n",
    "\n",
    "import gc\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.dialects import registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b49b",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[3. Connecting In Snowflake](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5171891",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(\n",
    "    user='*******',\n",
    "    password='*******',\n",
    "    account='*******',\n",
    "    warehouse='*******',\n",
    "    database='*******',\n",
    "    schema='*******',\n",
    "    role = '*******'\n",
    ")\n",
    "\n",
    "\n",
    "registry.register('snowflake', 'snowflake.sqlalchemy', 'dialect')\n",
    "\n",
    "engine = create_engine(\n",
    "    'snowflake://{user}:{password}@{account}/{db}/{schema}?warehouse={warehouse}'.format(\n",
    "        user='*******',\n",
    "        password='*******',\n",
    "        account='*******',\n",
    "        warehouse='*******',\n",
    "        db='*******',\n",
    "        schema='*******'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6d0d8",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[4. Defining Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12c2c2",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[4.1. Text Cleaning](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f257a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    # Remove HTML tags using BeautifulSoup\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text(separator=\" \")\n",
    "    \n",
    "    # Remove patterns like \\n and its variations\n",
    "    cleaned_text = re.sub(r'\\\\n+', ' ', cleaned_text)\n",
    "    \n",
    "    # Remove punctuations (excluding single quote to preserve words like \"isn't\" and \"it's\")\n",
    "    cleaned_text = re.sub(r'[^\\w\\s\\']', ' ', cleaned_text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "    # Removing t, tt and its similarities\n",
    "    cleaned_text = re.sub(r'\\bt+\\b', ' ', cleaned_text)\n",
    "\n",
    "    cleaned_text = re.sub(r'\\btt+\\b', ' ', cleaned_text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    # Strip leading and trailing whitespaces\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    #all text to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Register the UDF\n",
    "# clean_html_udf = udf(clean_html, StringType())\n",
    "\n",
    "# Register the UDF\n",
    "# clean_text_udf = udf(clean_html_udf, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd42a6",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[4.2. Tokenizing](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e566e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(column):\n",
    "    \"\"\"Tokenizes a Pandas dataframe column and returns a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        column: Pandas dataframe column (i.e. df['text']).\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): Tokenized list\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454c98e",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[4.3. Remove Stopwords](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f6ec03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = list(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e8252cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StopWordsRemover(text):\n",
    "    '''\n",
    "    Removes Stop Words (also capitalized) from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without Stop Words\n",
    "    ''' \n",
    "    # check in lowercase \n",
    "    t = [token for token in text if token.lower() not in stopwords]\n",
    "    text = ' '.join(t)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc67b9a",
   "metadata": {},
   "source": [
    "## <a id='toc4_4_'></a>[4.4. Count Words in each product and discard irrelevant products](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b22f7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_func(text):\n",
    "    '''\n",
    "    Counts words within a string\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Number of words within a string, integer\n",
    "    ''' \n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a532c3b",
   "metadata": {},
   "source": [
    "## <a id='toc4_5_'></a>[4.5. Count Vectorizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5611308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_MIN = 1\n",
    "NGRAM_MAX = 1\n",
    "MIN_DOC_FREQ = 1\n",
    "MAX_DOC_FREQ = 1.0\n",
    "MAX_TERMS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdc97a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(NGRAM_MIN,NGRAM_MAX), stop_words=None, \n",
    "                             lowercase=False, max_df=MAX_DOC_FREQ, min_df=MIN_DOC_FREQ, \n",
    "                             max_features=MAX_TERMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae995f",
   "metadata": {},
   "source": [
    "## <a id='toc4_6_'></a>[4.6. One Hot Encoding](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c562681",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73da8b",
   "metadata": {},
   "source": [
    "## <a id='toc4_7_'></a>[4.7. Recommendations with WALS](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19a67796",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 16\n",
    "MIN_WORDS = 32\n",
    "N_COMPONENTS = 256\n",
    "N_CLUSTERS = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf9749ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wals = AlternatingLeastSquares(factors=N_COMPONENTS, regularization=0.1,\n",
    "                               iterations=15, calculate_training_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310af3f1",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[5. Loading Data and Applying all Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6c591",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[5.1. Extracting list of all Main Categories](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "771e304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    MAIN_CATEGORY\n",
    "FROM \n",
    "    PRODUCTS\n",
    "WHERE \n",
    "    TITLE <> '[]' AND \n",
    "    PRICE IS NOT NULL AND\n",
    "    MAIN_CATEGORY IS NOT NULL AND \n",
    "    MAIN_CATEGORY <> ''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a015d1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/__/yd2xb0114nl_x78dddwtn9k00000gn/T/ipykernel_37706/3386730848.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  category = pd.read_sql_query(query1, conn)\n"
     ]
    }
   ],
   "source": [
    "category = pd.read_sql_query(query1, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff223859",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = category.MAIN_CATEGORY.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3112eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Movies & Tv',\n",
       " 'Software',\n",
       " 'Buy A Kindle',\n",
       " 'Amazon Fashion',\n",
       " 'Computers',\n",
       " 'Arts, Crafts & Sewing',\n",
       " 'Automotive',\n",
       " 'Office Products',\n",
       " 'All Electronics',\n",
       " 'Baby',\n",
       " 'Camera & Photo',\n",
       " 'Video Games',\n",
       " 'Apple Products',\n",
       " 'Health & Personal Care',\n",
       " 'Appliances',\n",
       " 'Sports Collectibles',\n",
       " 'Handmade',\n",
       " 'Home & Business Services',\n",
       " 'Industrial & Scientific',\n",
       " 'Portable Audio & Accessories',\n",
       " 'Collectible Coins',\n",
       " 'Cell Phones & Accessories',\n",
       " 'Gift Cards',\n",
       " 'Pet Supplies',\n",
       " 'Books',\n",
       " 'Home Audio & Theater',\n",
       " 'Tools & Home Improvement',\n",
       " 'Gps & Navigation',\n",
       " 'Toys & Games',\n",
       " 'Musical Instruments',\n",
       " 'Digital Music',\n",
       " 'All Beauty',\n",
       " 'Prime Pantry',\n",
       " 'Sports & Outdoors',\n",
       " 'Fire Phone',\n",
       " 'Memberships & Subscriptions',\n",
       " 'Luxury Beauty',\n",
       " 'Amazon Devices',\n",
       " 'Grocery',\n",
       " 'Amazon Home',\n",
       " 'Collectibles & Fine Art',\n",
       " 'Amazon Fire Tv',\n",
       " 'Car Electronics',\n",
       " 'Entertainment']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b4bf9",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[5.2. Producing a Recommendation dataframe to each Main Category](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce922d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arts, Crafts & Sewing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/__/yd2xb0114nl_x78dddwtn9k00000gn/T/ipykernel_37706/1061392048.py:42: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  metadata = pd.read_sql_query(query, conn)\n",
      "/var/folders/__/yd2xb0114nl_x78dddwtn9k00000gn/T/ipykernel_37706/1491938022.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "/Users/jessica/Library/Python/3.9/lib/python/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "\u001b[32m2023-08-05 15:55:47.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mFactorizing with WALS...\u001b[0m\n",
      "/Users/jessica/Library/Python/3.9/lib/python/site-packages/implicit/utils.py:138: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.00011587142944335938 seconds\n",
      "  warnings.warn(\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in category_list:\n",
    "    query = \"\"\"\n",
    "    WITH CTE AS (\n",
    "        SELECT \n",
    "            PROD.ASIN,\n",
    "            PROD.TITLE,\n",
    "            PROD.BRAND,\n",
    "            DET.DETAILS,\n",
    "            REL.RELATED_CATEGORIES,\n",
    "            PROD.PRICE,\n",
    "            REV.OVERALL,\n",
    "            AVG(REV.OVERALL) OVER (PARTITION BY PROD.ASIN) AS \"PRODUCT_AVG_RATING\" \n",
    "        FROM \n",
    "            PRODUCTS AS PROD \n",
    "        INNER JOIN \n",
    "            PRODUCTS_REVIEWS AS REV ON PROD.ASIN = REV.ASIN \n",
    "        INNER JOIN \n",
    "            PRODUCTS_DETAILS AS DET ON PROD.ASIN = DET.ASIN\n",
    "        INNER JOIN \n",
    "            PRODUCTS_RELATED AS REL ON PROD.ASIN = REL.ASIN\n",
    "        WHERE \n",
    "            PROD.MAIN_CATEGORY = '{i}' AND \n",
    "            PROD.TITLE <> '' AND\n",
    "            PROD.TITLE IS NOT NULL AND\n",
    "            PROD.PRICE IS NOT NULL\n",
    "    )\n",
    "    SELECT\n",
    "        *\n",
    "    FROM \n",
    "        (\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY ASIN ORDER BY PRICE DESC) AS ROW_NUMBER\n",
    "    FROM\n",
    "        CTE\n",
    "    )\n",
    "    WHERE \n",
    "        ROW_NUMBER = 1 AND \n",
    "        PRODUCT_AVG_RATING >= 4\n",
    "    \"\"\".format(i=i)\n",
    "    print(i)\n",
    "    metadata = pd.read_sql_query(query, conn)\n",
    "\n",
    "    cols = ['TITLE', 'BRAND', 'RELATED_CATEGORIES', 'DETAILS']\n",
    "    metadata['full_text'] = metadata[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    metadata['full_text'] = metadata['full_text'].apply(clean_html)\n",
    "    metadata['text_tokens'] = metadata['full_text'].apply(tokenize)\n",
    "    metadata['text_tokens_sw'] = metadata['text_tokens'].apply(StopWordsRemover)\n",
    "    metadata['word_count'] = metadata['text_tokens_sw'].apply(word_count_func)\n",
    "    metadata = metadata[metadata['word_count'] > 32]  \n",
    "    if len(metadata) > 10:\n",
    "        metadata.reset_index(level=None, drop=True, inplace=True, col_level=0, col_fill='') ## reseting index\n",
    "        x_count = vectorizer.fit_transform(metadata['text_tokens_sw'])\n",
    "        x_wt = bm25_weight(x_count)\n",
    "        x_tags = enc.fit_transform(metadata[['BRAND']])\n",
    "        x_tags.data = 5.0 * x_tags.data\n",
    "        x = hstack([x_wt, x_tags])\n",
    "\n",
    "        logger.info('Factorizing with WALS...')\n",
    "        wals.fit(x, show_progress=True)\n",
    "        doc_factors_wals = normalize(wals.item_factors, norm=\"l2\", axis=1, copy=False)\n",
    "        word_factors_wals = normalize(wals.user_factors, norm=\"l2\", axis=1, copy=False)\n",
    "\n",
    "        logger.info('Finding event nearest neighbors with WALS factors...')\n",
    "        K = 16 # number of recommendations for each product\n",
    "        published_idx = metadata.index.values\n",
    "        original_idx = published_idx\n",
    "        published_idx = np.array(list(range(0, len(original_idx))))\n",
    "        pub_evts = IndexFlatIP(N_COMPONENTS)\n",
    "        pub_evts.add(word_factors_wals[published_idx])\n",
    "        nn_dist, nn_idx = pub_evts.search(word_factors_wals, K+1)\n",
    "        product_neighbor = pd.DataFrame.from_dict({\n",
    "        'product_id': np.repeat(metadata['ASIN'].values, K+1),\n",
    "        'num_words': np.repeat(metadata['word_count'].values, K+1),\n",
    "        'num_words_neighbor': metadata['word_count'].values[published_idx[nn_idx.flatten()]],\n",
    "        'neighbor_id': metadata['ASIN'].values[published_idx[nn_idx.flatten()]], \n",
    "        'similarity': nn_dist.flatten()\n",
    "        })\n",
    "        same_id = product_neighbor.loc[product_neighbor['product_id'] == product_neighbor['neighbor_id']].index.values.tolist()\n",
    "        product_neighbor.drop(labels=same_id, axis='index', inplace=True)\n",
    "        product_neighbor.sort_values(['product_id','similarity'], ascending=[True, False], inplace=True)\n",
    "\n",
    "        recommendations = product_neighbor.groupby('product_id')['neighbor_id'].apply(list).reset_index(name='recommendations')\n",
    "\n",
    "        recommendations['MAIN_CATEGORY'] = '{i}'.format(i=i)\n",
    "\n",
    "        recommendations.to_parquet(\n",
    "            path=\"../output/similarity/recommendations_{i}.parquet\".format(i=i),\n",
    "            engine=\"auto\"\n",
    "        )   \n",
    "        del metadata, x_count, x_wt, x_tags, x, doc_factors_wals, word_factors_wals, published_idx, original_idx, pub_evts, nn_dist, nn_idx, product_neighbor, same_id, recommendations \n",
    "        gc.collect()\n",
    "    else:\n",
    "        pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
