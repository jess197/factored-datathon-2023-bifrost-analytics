{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52bdacb",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [1. About the Notebook](#toc1_)    \n",
    "- [2. Libraries and Packages](#toc2_)    \n",
    "- [3. Connecting In Snowflake](#toc3_)    \n",
    "- [4. Defining Functions](#toc4_)    \n",
    "  - [4.1. Text Cleaning](#toc4_1_)    \n",
    "  - [4.2. Tokenizing](#toc4_2_)    \n",
    "  - [4.3. Remove Stopwords](#toc4_3_)    \n",
    "  - [4.4. Count Words in each product and discard irrelevant products](#toc4_4_)    \n",
    "  - [4.5. Count Vectorizer](#toc4_5_)    \n",
    "  - [4.6. One Hot Encoding](#toc4_6_)    \n",
    "  - [4.7. Recommendations with WALS](#toc4_7_)    \n",
    "- [5. Loading Data and Applying all Functions](#toc5_)    \n",
    "  - [5.1. Extracting list of all Main Categories](#toc5_1_)    \n",
    "  - [5.2. Producing a Recommendation dataframe to each Main Category](#toc5_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1527991b",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[1. About the Notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22eb9a",
   "metadata": {},
   "source": [
    "Notebook that creates similarity recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f43108",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[2. Libraries and Packages](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb274799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunnokalyxton/anaconda3/lib/python3.9/site-packages/snowflake/connector/options.py:103: UserWarning: You have an incompatible version of 'pyarrow' installed (8.0.0), please install a version that adheres to: 'pyarrow<10.1.0,>=10.0.1; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/brunnokalyxton/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import sigmoid_kernel\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import spacy\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "from faiss import IndexFlatIP\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.examples import sentences \n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b49b",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[3. Connecting In Snowflake](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5171891",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(\n",
    "    user='************',\n",
    "    password='************',\n",
    "    account='*************',\n",
    "    warehouse='ANALYTICS_WH',\n",
    "    database='AMAZON',\n",
    "    schema='AMZ_DATA_GOLD',\n",
    "    role = 'ANALYSTS'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6d0d8",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[4. Defining Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12c2c2",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[4.1. Text Cleaning](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f257a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    # Remove HTML tags using BeautifulSoup\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text(separator=\" \")\n",
    "    \n",
    "    # Remove patterns like \\n and its variations\n",
    "    cleaned_text = re.sub(r'\\\\n+', ' ', cleaned_text)\n",
    "    \n",
    "    # Remove punctuations (excluding single quote to preserve words like \"isn't\" and \"it's\")\n",
    "    cleaned_text = re.sub(r'[^\\w\\s\\']', ' ', cleaned_text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "    # Removing t, tt and its similarities\n",
    "    cleaned_text = re.sub(r'\\bt+\\b', ' ', cleaned_text)\n",
    "\n",
    "    cleaned_text = re.sub(r'\\btt+\\b', ' ', cleaned_text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    # Strip leading and trailing whitespaces\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    #all text to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Register the UDF\n",
    "# clean_html_udf = udf(clean_html, StringType())\n",
    "\n",
    "# Register the UDF\n",
    "# clean_text_udf = udf(clean_html_udf, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd42a6",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[4.2. Tokenizing](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e566e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(column):\n",
    "    \"\"\"Tokenizes a Pandas dataframe column and returns a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        column: Pandas dataframe column (i.e. df['text']).\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): Tokenized list\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454c98e",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[4.3. Remove Stopwords](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6ec03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = list(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8252cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StopWordsRemover(text):\n",
    "    '''\n",
    "    Removes Stop Words (also capitalized) from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without Stop Words\n",
    "    ''' \n",
    "    # check in lowercase \n",
    "    t = [token for token in text if token.lower() not in stopwords]\n",
    "    text = ' '.join(t)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc67b9a",
   "metadata": {},
   "source": [
    "## <a id='toc4_4_'></a>[4.4. Count Words in each product and discard irrelevant products](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22f7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_func(text):\n",
    "    '''\n",
    "    Counts words within a string\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Number of words within a string, integer\n",
    "    ''' \n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a532c3b",
   "metadata": {},
   "source": [
    "## <a id='toc4_5_'></a>[4.5. Count Vectorizer](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5611308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_MIN = 1\n",
    "NGRAM_MAX = 1\n",
    "MIN_DOC_FREQ = 1\n",
    "MAX_DOC_FREQ = 1.0\n",
    "MAX_TERMS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc97a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(NGRAM_MIN,NGRAM_MAX), stop_words=None, \n",
    "                             lowercase=False, max_df=MAX_DOC_FREQ, min_df=MIN_DOC_FREQ, \n",
    "                             max_features=MAX_TERMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae995f",
   "metadata": {},
   "source": [
    "## <a id='toc4_6_'></a>[4.6. One Hot Encoding](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c562681",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73da8b",
   "metadata": {},
   "source": [
    "## <a id='toc4_7_'></a>[4.7. Recommendations with WALS](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a67796",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_THREADS = 16\n",
    "MIN_WORDS = 32\n",
    "N_COMPONENTS = 256\n",
    "N_CLUSTERS = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf9749ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunnokalyxton/anaconda3/lib/python3.9/site-packages/implicit/utils.py:33: UserWarning: Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wals = AlternatingLeastSquares(factors=N_COMPONENTS, regularization=0.1,\n",
    "                               iterations=15, calculate_training_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310af3f1",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[5. Loading Data and Applying all Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6c591",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[5.1. Extracting list of all Main Categories](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "771e304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    MAIN_CATEGORY\n",
    "FROM \n",
    "    PRODUCTS\n",
    "WHERE \n",
    "    TITLE <> '[]' AND \n",
    "    PRICE IS NOT NULL AND\n",
    "    MAIN_CATEGORY IS NOT NULL AND \n",
    "    MAIN_CATEGORY <> ''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a015d1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10802/3386730848.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  category = pd.read_sql_query(query1, conn)\n"
     ]
    }
   ],
   "source": [
    "category = pd.read_sql_query(query1, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff223859",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = category.MAIN_CATEGORY.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a6fbb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = ['All Beauty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3112eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Movies & Tv',\n",
       " 'Portable Audio & Accessories',\n",
       " 'Software',\n",
       " 'Buy A Kindle',\n",
       " 'Amazon Fashion',\n",
       " 'Collectible Coins',\n",
       " 'Computers',\n",
       " 'Cell Phones & Accessories',\n",
       " 'Gift Cards',\n",
       " 'Arts, Crafts & Sewing',\n",
       " 'Pet Supplies',\n",
       " 'Automotive',\n",
       " 'Office Products',\n",
       " 'Books',\n",
       " 'All Electronics',\n",
       " 'Tools & Home Improvement',\n",
       " 'Home Audio & Theater',\n",
       " 'Baby',\n",
       " 'Gps & Navigation',\n",
       " 'Toys & Games',\n",
       " 'Camera & Photo',\n",
       " 'Musical Instruments',\n",
       " 'Video Games',\n",
       " 'Digital Music',\n",
       " 'Sports & Outdoors',\n",
       " 'All Beauty',\n",
       " 'Prime Pantry',\n",
       " 'Fire Phone',\n",
       " 'Memberships & Subscriptions',\n",
       " 'Amazon Devices',\n",
       " 'Luxury Beauty',\n",
       " 'Apple Products',\n",
       " 'Health & Personal Care',\n",
       " 'Grocery',\n",
       " 'Appliances',\n",
       " 'Amazon Home',\n",
       " 'Collectibles & Fine Art',\n",
       " 'Sports Collectibles',\n",
       " 'Amazon Fire Tv',\n",
       " 'Handmade',\n",
       " 'Home & Business Services',\n",
       " 'Industrial & Scientific',\n",
       " 'Car Electronics',\n",
       " 'Entertainment']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efd69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "032b4bf9",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[5.2. Producing a Recommendation dataframe to each Main Category](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce922d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Beauty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10802/1061392048.py:42: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  metadata = pd.read_sql_query(query, conn)\n",
      "/home/brunnokalyxton/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "2023-08-05 13:05:44.108 | INFO     | __main__:<module>:60 - Factorizing with WALS...\n",
      "/home/brunnokalyxton/anaconda3/lib/python3.9/site-packages/implicit/utils.py:138: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.0005609989166259766 seconds\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264505152b69443f833b909cf6aeab00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModelFitError",
     "evalue": "NaN encountered in factors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelFitError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10802/1061392048.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Factorizing with WALS...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mwals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mdoc_factors_wals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_factors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mword_factors_wals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_factors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/implicit/cpu/als.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, user_items, show_progress, callback)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final training loss %.4f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecalculate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/implicit/cpu/matrix_factorization_base.py\u001b[0m in \u001b[0;36m_check_fit_errors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mis_nan\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mModelFitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NaN encountered in factors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelFitError\u001b[0m: NaN encountered in factors"
     ]
    }
   ],
   "source": [
    "for i in category_list:\n",
    "    query = \"\"\"\n",
    "    WITH CTE AS (\n",
    "        SELECT \n",
    "            PROD.ASIN,\n",
    "            PROD.TITLE,\n",
    "            PROD.BRAND,\n",
    "            DET.DETAILS,\n",
    "            REL.RELATED_CATEGORIES,\n",
    "            PROD.PRICE,\n",
    "            REV.OVERALL,\n",
    "            AVG(REV.OVERALL) OVER (PARTITION BY PROD.ASIN) AS \"PRODUCT_AVG_RATING\" \n",
    "        FROM \n",
    "            PRODUCTS AS PROD \n",
    "        INNER JOIN \n",
    "            PRODUCTS_REVIEWS AS REV ON PROD.ASIN = REV.ASIN \n",
    "        INNER JOIN \n",
    "            PRODUCTS_DETAILS AS DET ON PROD.ASIN = DET.ASIN\n",
    "        INNER JOIN \n",
    "            PRODUCTS_RELATED AS REL ON PROD.ASIN = REL.ASIN\n",
    "        WHERE \n",
    "            PROD.MAIN_CATEGORY = '{i}' AND \n",
    "            PROD.TITLE <> '' AND\n",
    "            PROD.TITLE IS NOT NULL AND\n",
    "            PROD.PRICE IS NOT NULL\n",
    "    )\n",
    "    SELECT\n",
    "        *\n",
    "    FROM \n",
    "        (\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY ASIN ORDER BY PRICE DESC) AS ROW_NUMBER\n",
    "    FROM\n",
    "        CTE\n",
    "    )\n",
    "    WHERE \n",
    "        ROW_NUMBER = 1 AND \n",
    "        PRODUCT_AVG_RATING >= 4\n",
    "    \"\"\".format(i=i)\n",
    "    print(i)\n",
    "    metadata = pd.read_sql_query(query, conn)\n",
    "\n",
    "    cols = ['TITLE', 'BRAND', 'RELATED_CATEGORIES', 'DETAILS']\n",
    "    metadata['full_text'] = metadata[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    metadata['full_text'] = metadata['full_text'].apply(clean_html)\n",
    "    metadata['text_tokens'] = metadata['full_text'].apply(tokenize)\n",
    "    metadata['text_tokens_sw'] = metadata['text_tokens'].apply(StopWordsRemover)\n",
    "    metadata['word_count'] = metadata['text_tokens_sw'].apply(word_count_func)\n",
    "    metadata = metadata[metadata['word_count'] > 32]  \n",
    "    if len(metadata) > 10:\n",
    "        metadata.reset_index(level=None, drop=True, inplace=True, col_level=0, col_fill='') ## reseting index\n",
    "        x_count = vectorizer.fit_transform(metadata['text_tokens_sw'])\n",
    "        x_wt = bm25_weight(x_count)\n",
    "        x_tags = enc.fit_transform(metadata[['BRAND']])\n",
    "        x_tags.data = 5.0 * x_tags.data\n",
    "        x = hstack([x_wt, x_tags])\n",
    "\n",
    "        logger.info('Factorizing with WALS...')\n",
    "        wals.fit(x, show_progress=True)\n",
    "        doc_factors_wals = normalize(wals.item_factors, norm=\"l2\", axis=1, copy=False)\n",
    "        word_factors_wals = normalize(wals.user_factors, norm=\"l2\", axis=1, copy=False)\n",
    "\n",
    "        logger.info('Finding event nearest neighbors with WALS factors...')\n",
    "        K = 16 # number of recommendations for each product\n",
    "        published_idx = metadata.index.values\n",
    "        original_idx = published_idx\n",
    "        published_idx = np.array(list(range(0, len(original_idx))))\n",
    "        pub_evts = IndexFlatIP(N_COMPONENTS)\n",
    "        pub_evts.add(word_factors_wals[published_idx])\n",
    "        nn_dist, nn_idx = pub_evts.search(word_factors_wals, K+1)\n",
    "        product_neighbor = pd.DataFrame.from_dict({\n",
    "        'product_id': np.repeat(metadata['ASIN'].values, K+1),\n",
    "        'num_words': np.repeat(metadata['word_count'].values, K+1),\n",
    "        'num_words_neighbor': metadata['word_count'].values[published_idx[nn_idx.flatten()]],\n",
    "        'neighbor_id': metadata['ASIN'].values[published_idx[nn_idx.flatten()]], \n",
    "        'similarity': nn_dist.flatten()\n",
    "        })\n",
    "        same_id = product_neighbor.loc[product_neighbor['product_id'] == product_neighbor['neighbor_id']].index.values.tolist()\n",
    "        product_neighbor.drop(labels=same_id, axis='index', inplace=True)\n",
    "        product_neighbor.sort_values(['product_id','similarity'], ascending=[True, False], inplace=True)\n",
    "\n",
    "        recommendations = product_neighbor.groupby('product_id')['neighbor_id'].apply(list).reset_index(name='recommendations')\n",
    "\n",
    "        recommendations['MAIN_CATEGORY'] = '{i}'.format(i=i)\n",
    "\n",
    "        recommendations.to_parquet(\n",
    "            path=\"../output/similarity/recommendations_{i}.parquet\".format(i=i),\n",
    "            engine=\"auto\"\n",
    "        )   \n",
    "        del metadata, x_count, x_wt, x_tags, x, doc_factors_wals, word_factors_wals, published_idx, original_idx, pub_evts, nn_dist, nn_idx, product_neighbor, same_id, recommendations \n",
    "        gc.collect()\n",
    "    else:\n",
    "        pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
